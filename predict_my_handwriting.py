"""
æ‰‹å†™æ•°å­—é¢„æµ‹ç³»ç»Ÿ
ç”¨äºé¢„å¤„ç†å’Œé¢„æµ‹è‡ªå·±çš„æ‰‹å†™æ•°å­—å›¾ç‰‡

æ ¸å¿ƒé—®é¢˜ï¼š
- æ‚¨çš„æ‰‹å†™å›¾ç‰‡ï¼šæµ…è‰²èƒŒæ™¯ã€é»‘è‰²æ•°å­—ã€å¤§å°ºå¯¸ã€ç»†çº¿æ¡
- MNISTæ ¼å¼ï¼šé»‘è‰²èƒŒæ™¯ã€ç™½è‰²æ•°å­—ã€28Ã—28ã€è¾ƒç²—çº¿æ¡

é¢„å¤„ç†æµç¨‹ï¼š
1. é¢œè‰²åè½¬ (ç™½åº•é»‘å­— â†’ é»‘åº•ç™½å­—)
2. å¯¹æ¯”åº¦å¢å¼º
3. å»å™ª
4. äºŒå€¼åŒ–
5. ç¬”ç”»å¢ç²— (è†¨èƒ€æ“ä½œ)
6. è£å‰ªæ•°å­—åŒºåŸŸ
7. ä¿æŒçºµæ¨ªæ¯”ç¼©æ”¾åˆ°20Ã—20
8. è´¨å¿ƒå±…ä¸­åˆ°28Ã—28ç”»å¸ƒ
9. é«˜æ–¯æ¨¡ç³Šå¹³æ»‘
10. å½’ä¸€åŒ–åˆ°[0,1]
"""

import tensorflow as tf
import numpy as np
import matplotlib.pyplot as plt
import cv2
import os
from sklearn.metrics import classification_report, confusion_matrix
import seaborn as sns

# è®¾ç½®ä¸­æ–‡æ˜¾ç¤º
plt.rcParams['font.sans-serif'] = ['SimHei', 'Microsoft YaHei', 'DejaVu Sans']
plt.rcParams['axes.unicode_minus'] = False


class HandwritingPredictor:
    """æ‰‹å†™æ•°å­—é¢„æµ‹å™¨"""
    
    def __init__(self, model_path='mnist_cnn_model.h5', data_dir='my_handwriting_digits'):
        """
        åˆå§‹åŒ–é¢„æµ‹å™¨
        
        Args:
            model_path: è®­ç»ƒå¥½çš„æ¨¡å‹è·¯å¾„
            data_dir: æ‰‹å†™æ•°å­—å›¾ç‰‡ç›®å½•
        """
        self.model_path = model_path
        self.data_dir = data_dir
        self.model = None
        
    def load_model(self):
        """åŠ è½½è®­ç»ƒå¥½çš„æ¨¡å‹"""
        if not os.path.exists(self.model_path):
            print(f"âŒ æ¨¡å‹æ–‡ä»¶ä¸å­˜åœ¨: {self.model_path}")
            return False
        
        try:
            print(f"ğŸ“¦ åŠ è½½æ¨¡å‹: {self.model_path}")
            self.model = tf.keras.models.load_model(self.model_path)
            print("âœ… æ¨¡å‹åŠ è½½æˆåŠŸ!")
            return True
        except Exception as e:
            print(f"âŒ æ¨¡å‹åŠ è½½å¤±è´¥: {e}")
            return False
    
    def preprocess_image(self, image_path, show_steps=False):
        """
        é¢„å¤„ç†æ‰‹å†™æ•°å­—å›¾ç‰‡ï¼Œè½¬æ¢ä¸ºMNISTæ ¼å¼
        
        å¤„ç†æµç¨‹ï¼š
        1. å…ˆåœ¨åŸå›¾ä¸Šæ‰¾åˆ°æ•°å­—è½®å»“ä½ç½®
        2. è£å‰ªå‡ºæ•°å­—åŒºåŸŸ
        3. åœ¨è£å‰ªåŒºåŸŸå†…è¿›è¡ŒäºŒå€¼åŒ–å’Œè†¨èƒ€åŠ ç²—
        4. ç¼©æ”¾å¹¶å±…ä¸­åˆ°28Ã—28
        
        Args:
            image_path: å›¾ç‰‡è·¯å¾„
            show_steps: æ˜¯å¦æ˜¾ç¤ºæ¯ä¸€æ­¥çš„å¤„ç†ç»“æœ
            
        Returns:
            å¤„ç†åçš„28Ã—28å½’ä¸€åŒ–å›¾åƒï¼Œæˆ–Noneï¼ˆå¤±è´¥æ—¶ï¼‰
        """
        # ========== æ­¥éª¤1: è¯»å–å›¾åƒ ==========
        img = cv2.imread(image_path, cv2.IMREAD_GRAYSCALE)
        if img is None:
            print(f"âš ï¸ æ— æ³•è¯»å–: {image_path}")
            return None
        
        steps = {'1.åŸå§‹å›¾åƒ': img.copy()} if show_steps else {}
        
        # ========== æ­¥éª¤2: å…ˆæ‰¾æ•°å­—è½®å»“ä½ç½® ==========
        # ä½¿ç”¨é«˜æ–¯æ¨¡ç³Šå‡å°‘å™ªå£°
        blurred = cv2.GaussianBlur(img, (5, 5), 0)
        
        # åˆ¤æ–­æ˜¯ç™½åº•é»‘å­—è¿˜æ˜¯é»‘åº•ç™½å­—
        is_light_bg = np.mean(img) > 127
        
        # ä½¿ç”¨OtsuäºŒå€¼åŒ–æ‰¾è½®å»“ï¼ˆç™½åº•é»‘å­—ç”¨THRESH_BINARY_INVä½¿æ•°å­—å˜ç™½ï¼‰
        if is_light_bg:
            _, thresh_for_contour = cv2.threshold(blurred, 0, 255, 
                                                   cv2.THRESH_BINARY_INV + cv2.THRESH_OTSU)
        else:
            _, thresh_for_contour = cv2.threshold(blurred, 0, 255,
                                                   cv2.THRESH_BINARY + cv2.THRESH_OTSU)
        
        if show_steps:
            steps['2.è½®å»“æ£€æµ‹ç”¨'] = thresh_for_contour.copy()
        
        # ========== æ­¥éª¤3: æ‰¾åˆ°æœ€å¤§è½®å»“ï¼ˆæ•°å­—ï¼‰ ==========
        contours, _ = cv2.findContours(thresh_for_contour, cv2.RETR_EXTERNAL, 
                                        cv2.CHAIN_APPROX_SIMPLE)
        
        if not contours:
            print(f"âš ï¸ æœªæ‰¾åˆ°è½®å»“: {image_path}")
            return None
        
        # æ‰¾æœ€å¤§è½®å»“ï¼ˆå‡è®¾æ˜¯æ•°å­—ï¼‰
        max_contour = max(contours, key=cv2.contourArea)
        x, y, w, h = cv2.boundingRect(max_contour)
        
        # æ·»åŠ è¾¹è·
        margin = int(max(w, h) * 0.15)
        x = max(0, x - margin)
        y = max(0, y - margin)
        w = min(img.shape[1] - x, w + 2 * margin)
        h = min(img.shape[0] - y, h + 2 * margin)
        
        if show_steps:
            # åœ¨åŸå›¾ä¸Šç”»å‡ºæ£€æµ‹åˆ°çš„åŒºåŸŸ
            img_with_rect = cv2.cvtColor(img, cv2.COLOR_GRAY2BGR)
            cv2.rectangle(img_with_rect, (x, y), (x+w, y+h), (0, 255, 0), 3)
            steps['3.æ£€æµ‹åŒºåŸŸ'] = img_with_rect
        
        # ========== æ­¥éª¤4: è£å‰ªæ•°å­—åŒºåŸŸ ==========
        cropped = img[y:y+h, x:x+w]
        
        if show_steps:
            steps['4.è£å‰ªåŒºåŸŸ'] = cropped.copy()
        
        # ========== æ­¥éª¤5: åœ¨è£å‰ªåŒºåŸŸå†…è¿›è¡Œå¯¹æ¯”åº¦å¢å¼º ==========
        clahe = cv2.createCLAHE(clipLimit=2.0, tileGridSize=(4, 4))
        enhanced = clahe.apply(cropped)
        
        if show_steps:
            steps['5.å¯¹æ¯”åº¦å¢å¼º'] = enhanced.copy()
        
        # ========== æ­¥éª¤6: äºŒå€¼åŒ–ï¼ˆç™½åº•é»‘å­—â†’é»‘åº•ç™½å­—ï¼‰ ==========
        # å¯¹è£å‰ªåçš„å°åŒºåŸŸä½¿ç”¨Otsu
        if is_light_bg:
            # ç™½åº•é»‘å­—ï¼šä½¿ç”¨THRESH_BINARY_INVï¼Œæ•°å­—å˜ç™½ï¼ŒèƒŒæ™¯å˜é»‘
            _, binary = cv2.threshold(enhanced, 0, 255,
                                      cv2.THRESH_BINARY_INV + cv2.THRESH_OTSU)
        else:
            _, binary = cv2.threshold(enhanced, 0, 255,
                                      cv2.THRESH_BINARY + cv2.THRESH_OTSU)
        
        if show_steps:
            steps['6.äºŒå€¼åŒ–'] = binary.copy()
        
        # ========== æ­¥éª¤7: è†¨èƒ€åŠ ç²—ç¬”ç”» ==========
        # è¿™æ˜¯å…³é”®ï¼ç»†ç¬”ç”»éœ€è¦åŠ ç²—æ‰èƒ½åŒ¹é…MNIST
        # æ ¹æ®è£å‰ªåŒºåŸŸå¤§å°å’Œçºµæ¨ªæ¯”åŠ¨æ€è°ƒæ•´è†¨èƒ€å¼ºåº¦
        crop_size = max(cropped.shape)
        crop_h, crop_w = cropped.shape
        crop_aspect = crop_w / crop_h if crop_h > 0 else 1.0
        
        # æçª„æ•°å­—éœ€è¦æ›´å¤šè†¨èƒ€æ¥ä¿ç•™ç‰¹å¾
        if crop_aspect < 0.4:
            dilate_base = 5  # æçª„æ•°å­—ï¼šæ›´å¤šè†¨èƒ€
        elif crop_size > 500:
            dilate_base = 4  # å¤§å›¾éœ€è¦æ›´å¤šè†¨èƒ€
        elif crop_size > 300:
            dilate_base = 3
        else:
            dilate_base = 2
        
        kernel = cv2.getStructuringElement(cv2.MORPH_ELLIPSE, (3, 3))
        dilated = cv2.dilate(binary, kernel, iterations=dilate_base)
        
        if show_steps:
            steps['7.è†¨èƒ€åŠ ç²—'] = dilated.copy()
        
        # ========== æ­¥éª¤8: é—­è¿ç®—è¿æ¥æ–­ç¬” ==========
        # å¯¹äºæçª„æ•°å­—ï¼ˆå¦‚9ï¼‰ï¼Œä½¿ç”¨æ›´å¤§çš„é—­è¿ç®—æ ¸æ¥è¿æ¥é¡¶éƒ¨å¯èƒ½æ–­å¼€çš„åœ†å½¢
        if crop_aspect < 0.4:
            close_size = 9  # æçª„æ•°å­—éœ€è¦æ›´å¤§çš„é—­è¿ç®—æ¥è¿æ¥9é¡¶éƒ¨åœ†åœˆ
        else:
            close_size = 5
        kernel_close = cv2.getStructuringElement(cv2.MORPH_ELLIPSE, (close_size, close_size))
        closed = cv2.morphologyEx(dilated, cv2.MORPH_CLOSE, kernel_close)
        
        if show_steps:
            steps['8.é—­è¿ç®—'] = closed.copy()
        
        # ========== æ­¥éª¤9: å†æ¬¡è£å‰ªåˆ°æ•°å­—è¾¹ç•Œ ==========
        coords = cv2.findNonZero(closed)
        if coords is None:
            print(f"âš ï¸ å¤„ç†åæœªæ£€æµ‹åˆ°æ•°å­—: {image_path}")
            return None
        
        x2, y2, w2, h2 = cv2.boundingRect(coords)
        margin2 = max(2, int(min(w2, h2) * 0.1))
        x2 = max(0, x2 - margin2)
        y2 = max(0, y2 - margin2)
        w2 = min(closed.shape[1] - x2, w2 + 2 * margin2)
        h2 = min(closed.shape[0] - y2, h2 + 2 * margin2)
        
        digit = closed[y2:y2+h2, x2:x2+w2]
        
        if show_steps:
            steps['9.ç²¾ç¡®è£å‰ª'] = digit.copy()
        
        # ========== æ­¥éª¤10: ç¼©æ”¾åˆ°åˆé€‚å°ºå¯¸ ==========
        # é—®é¢˜åˆ†æï¼š
        # - æ‚¨å†™çš„4ã€7ã€9æ¯”è¾ƒç˜¦é•¿ï¼Œçºµæ¨ªæ¯”çº¦0.3-0.55
        # - ç¼©æ”¾ååªæœ‰8-9åƒç´ å®½ï¼Œå’Œæ•°å­—1(ä¹Ÿæ˜¯8åƒç´ å®½)å¾ˆåƒ
        # - MNISTçš„æ•°å­—é€šå¸¸æ›´"èƒ–"ä¸€äº›
        # 
        # è§£å†³æ–¹æ¡ˆï¼šå¯¹äºç˜¦é•¿æ•°å­—ï¼Œé€‚å½“å¢åŠ å®½åº¦ï¼Œä½†è¦åˆ†æƒ…å†µå¤„ç†
        
        h_d, w_d = digit.shape
        aspect_ratio = w_d / h_d
        
        # æ ¹æ®çºµæ¨ªæ¯”å†³å®šç›®æ ‡å°ºå¯¸
        if aspect_ratio < 0.35:
            # æçª„æ•°å­—(å¦‚æŸäº›9)ï¼šéœ€è¦æ›´å®½æ‰èƒ½ä¿æŒé¡¶éƒ¨åœ†å½¢çš„å¯è¾¨è¯†æ€§
            new_h = 20
            new_w = max(14, int(w_d * 20 / h_d))  # æçª„æ•°å­—ä¹Ÿç”¨14åƒç´ æœ€å°å®½åº¦
            new_w = min(new_w, 20)
        elif aspect_ratio < 0.6:
            # ç˜¦é•¿æ•°å­—ï¼šå›ºå®šé«˜åº¦20ï¼Œå®½åº¦æœ€å°14åƒç´ ï¼ˆç¡®ä¿ä¸ä¼šå¤ªçª„åƒ1ï¼‰
            new_h = 20
            new_w = max(14, int(w_d * 20 / h_d))
            new_w = min(new_w, 20)
        elif aspect_ratio > 2.0:
            # æ‰å®½æ•°å­—ï¼šå›ºå®šå®½åº¦20ï¼Œé«˜åº¦æŒ‰æ¯”ä¾‹
            new_w = 20
            new_h = max(10, int(h_d * 20 / w_d))
            new_h = min(new_h, 20)
        else:
            # æ­£å¸¸æ¯”ä¾‹ï¼šä¿æŒçºµæ¨ªæ¯”ç¼©æ”¾åˆ°20Ã—20å†…
            scale = min(20 / w_d, 20 / h_d)
            new_w = int(w_d * scale)
            new_h = int(h_d * scale)
        
        resized = cv2.resize(digit, (new_w, new_h), interpolation=cv2.INTER_AREA)
        
        if show_steps:
            steps['10.ç¼©æ”¾'] = resized.copy()
        
        # ========== æ­¥éª¤11: å±…ä¸­åˆ°28Ã—28ç”»å¸ƒ ==========
        canvas = np.zeros((28, 28), dtype=np.uint8)
        x_offset = (28 - new_w) // 2
        y_offset = (28 - new_h) // 2
        canvas[y_offset:y_offset+new_h, x_offset:x_offset+new_w] = resized
        
        if show_steps:
            steps['11.å±…ä¸­28x28'] = canvas.copy()
        
        # ========== æ­¥éª¤12: é«˜æ–¯æ¨¡ç³Šå¹³æ»‘è¾¹ç¼˜ ==========
        blurred = cv2.GaussianBlur(canvas, (3, 3), 0.5)
        
        # ========== æ­¥éª¤13: å¢å¼ºäº®åº¦ä½¿å…¶åŒ¹é…MNIST ==========
        # é—®é¢˜ï¼šå¤„ç†ååƒç´ å€¼å¤ªä½(å‡å€¼0.02)ï¼ŒMNISTå‡å€¼çº¦0.1
        # è§£å†³ï¼šå¯¹éé›¶åŒºåŸŸè¿›è¡Œäº®åº¦å¢å¼º
        if np.max(blurred) > 0:
            # å°†æœ€å¤§å€¼æ‹‰ä¼¸åˆ°200-255èŒƒå›´ï¼Œæ¨¡æ‹ŸMNISTçš„äº®åº¦
            scale_factor = 220.0 / max(np.max(blurred), 1)
            final = np.clip(blurred * scale_factor, 0, 255).astype(np.uint8)
        else:
            final = blurred
        
        if show_steps:
            steps['12.äº®åº¦å¢å¼º'] = final.copy()
        
        # ========== å½’ä¸€åŒ–åˆ°[0,1] ==========
        normalized = final.astype('float32') / 255.0
        
        if show_steps:
            return normalized, steps
        
        return normalized
    
    def visualize_preprocessing(self, image_path):
        """å¯è§†åŒ–é¢„å¤„ç†çš„æ¯ä¸€æ­¥"""
        result = self.preprocess_image(image_path, show_steps=True)
        
        if result is None:
            return
        
        normalized, steps = result
        
        # åˆ›å»ºå¯è§†åŒ–
        n_steps = len(steps)
        cols = 4
        rows = (n_steps + cols - 1) // cols
        
        fig, axes = plt.subplots(rows, cols, figsize=(16, 4 * rows))
        axes = axes.flatten()
        
        for idx, (title, img) in enumerate(steps.items()):
            axes[idx].imshow(img, cmap='gray')
            axes[idx].set_title(title, fontsize=10)
            axes[idx].axis('off')
            
            # æ˜¾ç¤ºå°ºå¯¸
            h, w = img.shape[:2]
            axes[idx].text(0.02, 0.98, f'{w}Ã—{h}', transform=axes[idx].transAxes,
                          fontsize=8, va='top', color='red',
                          bbox=dict(boxstyle='round', facecolor='white', alpha=0.8))
        
        # éšè—å¤šä½™çš„å­å›¾
        for idx in range(n_steps, len(axes)):
            axes[idx].axis('off')
        
        plt.suptitle(f'é¢„å¤„ç†æ­¥éª¤: {os.path.basename(image_path)}', fontsize=14)
        plt.tight_layout()
        plt.show()
        
        return normalized
    
    def predict_single(self, image_path, show_details=False):
        """
        é¢„æµ‹å•å¼ å›¾ç‰‡
        
        Args:
            image_path: å›¾ç‰‡è·¯å¾„
            show_details: æ˜¯å¦æ˜¾ç¤ºè¯¦ç»†ä¿¡æ¯
            
        Returns:
            (é¢„æµ‹æ ‡ç­¾, ç½®ä¿¡åº¦, æ‰€æœ‰ç±»åˆ«æ¦‚ç‡)
        """
        if self.model is None:
            print("âŒ è¯·å…ˆåŠ è½½æ¨¡å‹")
            return None, None, None
        
        # é¢„å¤„ç†
        processed = self.preprocess_image(image_path)
        if processed is None:
            return None, None, None
        
        # é¢„æµ‹
        img_input = processed.reshape(1, 28, 28, 1)
        predictions = self.model.predict(img_input, verbose=0)[0]
        
        pred_label = np.argmax(predictions)
        confidence = predictions[pred_label]
        
        if show_details:
            print(f"\nğŸ“Š é¢„æµ‹è¯¦æƒ…: {os.path.basename(image_path)}")
            print(f"   é¢„æµ‹ç»“æœ: {pred_label}")
            print(f"   ç½®ä¿¡åº¦: {confidence:.4f} ({confidence*100:.2f}%)")
            print(f"   å„ç±»åˆ«æ¦‚ç‡:")
            for i, prob in enumerate(predictions):
                bar = 'â–ˆ' * int(prob * 20)
                print(f"      {i}: {prob:.4f} |{bar}")
        
        return pred_label, confidence, predictions
    
    def predict_batch(self, show_results=True):
        """
        æ‰¹é‡é¢„æµ‹ç›®å½•ä¸­çš„æ‰€æœ‰å›¾ç‰‡
        
        Returns:
            é¢„æµ‹ç»“æœåˆ—è¡¨, å‡†ç¡®ç‡
        """
        if self.model is None:
            print("âŒ è¯·å…ˆåŠ è½½æ¨¡å‹")
            return None, None
        
        if not os.path.exists(self.data_dir):
            print(f"âŒ ç›®å½•ä¸å­˜åœ¨: {self.data_dir}")
            return None, None
        
        # è·å–æ‰€æœ‰å›¾ç‰‡
        image_files = sorted([
            f for f in os.listdir(self.data_dir)
            if f.lower().endswith(('.jpg', '.jpeg', '.png', '.bmp'))
        ])
        
        if not image_files:
            print(f"âŒ æœªæ‰¾åˆ°å›¾ç‰‡æ–‡ä»¶")
            return None, None
        
        print(f"\nğŸ“ æ‰¾åˆ° {len(image_files)} å¼ å›¾ç‰‡")
        print("=" * 60)
        
        results = []
        correct = 0
        
        for filename in image_files:
            # ä»æ–‡ä»¶åæå–çœŸå®æ ‡ç­¾ (æ ¼å¼: æ•°å­—_ç¼–å·.jpg)
            try:
                true_label = int(filename.split('_')[0])
            except:
                print(f"âš ï¸ è·³è¿‡ {filename}: æ— æ³•è§£ææ ‡ç­¾")
                continue
            
            image_path = os.path.join(self.data_dir, filename)
            pred_label, confidence, probs = self.predict_single(image_path)
            
            if pred_label is None:
                continue
            
            is_correct = (pred_label == true_label)
            if is_correct:
                correct += 1
            
            results.append({
                'filename': filename,
                'true_label': true_label,
                'pred_label': pred_label,
                'confidence': confidence,
                'correct': is_correct,
                'probabilities': probs
            })
            
            # æ‰“å°ç»“æœ
            status = 'âœ“' if is_correct else 'âœ—'
            print(f"{status} {filename}: çœŸå®={true_label}, é¢„æµ‹={pred_label}, ç½®ä¿¡åº¦={confidence:.3f}")
        
        # è®¡ç®—å‡†ç¡®ç‡
        total = len(results)
        accuracy = correct / total if total > 0 else 0
        
        print("\n" + "=" * 60)
        print(f"ğŸ“Š é¢„æµ‹ç»“æœæ±‡æ€»")
        print(f"   æ€»æ ·æœ¬: {total}")
        print(f"   æ­£ç¡®æ•°: {correct}")
        print(f"   å‡†ç¡®ç‡: {accuracy:.4f} ({accuracy*100:.2f}%)")
        print("=" * 60)
        
        if show_results:
            self._show_results(results)
        
        return results, accuracy
    
    def _show_results(self, results):
        """å¯è§†åŒ–é¢„æµ‹ç»“æœ"""
        if not results:
            return
        
        # ç»Ÿè®¡æ¯ä¸ªæ•°å­—çš„å‡†ç¡®ç‡
        digit_stats = {}
        for i in range(10):
            digit_results = [r for r in results if r['true_label'] == i]
            if digit_results:
                correct = sum(1 for r in digit_results if r['correct'])
                digit_stats[i] = {
                    'total': len(digit_results),
                    'correct': correct,
                    'accuracy': correct / len(digit_results)
                }
        
        # ç»˜åˆ¶å„æ•°å­—å‡†ç¡®ç‡
        fig, axes = plt.subplots(1, 2, figsize=(14, 5))
        
        # æŸ±çŠ¶å›¾
        digits = list(digit_stats.keys())
        accuracies = [digit_stats[d]['accuracy'] for d in digits]
        colors = ['green' if acc >= 0.8 else 'orange' if acc >= 0.5 else 'red' for acc in accuracies]
        
        axes[0].bar(digits, accuracies, color=colors, edgecolor='black')
        axes[0].set_xlabel('æ•°å­—')
        axes[0].set_ylabel('å‡†ç¡®ç‡')
        axes[0].set_title('å„æ•°å­—è¯†åˆ«å‡†ç¡®ç‡')
        axes[0].set_ylim(0, 1.1)
        axes[0].axhline(y=0.8, color='green', linestyle='--', alpha=0.5, label='80%')
        axes[0].axhline(y=0.5, color='orange', linestyle='--', alpha=0.5, label='50%')
        
        for i, (d, acc) in enumerate(zip(digits, accuracies)):
            axes[0].text(d, acc + 0.02, f'{acc:.0%}', ha='center', fontsize=9)
        
        # æ··æ·†çŸ©é˜µ
        true_labels = [r['true_label'] for r in results]
        pred_labels = [r['pred_label'] for r in results]
        
        cm = confusion_matrix(true_labels, pred_labels, labels=range(10))
        sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', ax=axes[1],
                   xticklabels=range(10), yticklabels=range(10))
        axes[1].set_xlabel('é¢„æµ‹æ ‡ç­¾')
        axes[1].set_ylabel('çœŸå®æ ‡ç­¾')
        axes[1].set_title('æ··æ·†çŸ©é˜µ')
        
        plt.tight_layout()
        plt.show()
    
    def compare_with_mnist(self, image_path):
        """å°†é¢„å¤„ç†åçš„å›¾ç‰‡ä¸MNISTæ ·æœ¬å¯¹æ¯”"""
        result = self.preprocess_image(image_path, show_steps=False)
        if result is None:
            return
        
        # è·å–çœŸå®æ ‡ç­¾
        filename = os.path.basename(image_path)
        try:
            true_label = int(filename.split('_')[0])
        except:
            true_label = 0
        
        # åŠ è½½MNISTæ ·æœ¬
        (_, _), (x_test, y_test) = tf.keras.datasets.mnist.load_data()
        
        # æ‰¾åˆ°ç›¸åŒæ•°å­—çš„MNISTæ ·æœ¬
        indices = np.where(y_test == true_label)[0][:5]
        
        # å¯è§†åŒ–å¯¹æ¯”
        fig, axes = plt.subplots(2, 6, figsize=(15, 5))
        
        # ç¬¬ä¸€è¡Œ: åŸå›¾å’Œå¤„ç†å
        original = cv2.imread(image_path)
        axes[0, 0].imshow(cv2.cvtColor(original, cv2.COLOR_BGR2RGB))
        axes[0, 0].set_title('åŸå›¾')
        axes[0, 0].axis('off')
        
        axes[0, 1].imshow(result, cmap='gray')
        axes[0, 1].set_title('é¢„å¤„ç†å', color='green', fontweight='bold')
        axes[0, 1].axis('off')
        
        # MNISTæ ·æœ¬
        for i, idx in enumerate(indices[:4]):
            axes[0, 2 + i].imshow(x_test[idx], cmap='gray')
            axes[0, 2 + i].set_title(f'MNIST #{idx}')
            axes[0, 2 + i].axis('off')
        
        # ç¬¬äºŒè¡Œ: å·®å¼‚åˆ†æ
        axes[1, 0].text(0.5, 0.5, f'çœŸå®æ ‡ç­¾: {true_label}', ha='center', va='center', fontsize=14)
        axes[1, 0].axis('off')
        
        # é¢„æµ‹ç»“æœ
        if self.model is not None:
            pred_label, confidence, _ = self.predict_single(image_path)
            color = 'green' if pred_label == true_label else 'red'
            axes[1, 1].text(0.5, 0.5, f'é¢„æµ‹: {pred_label}\nç½®ä¿¡åº¦: {confidence:.2%}', 
                          ha='center', va='center', fontsize=12, color=color)
        axes[1, 1].axis('off')
        
        # æ˜¾ç¤ºä¸MNISTçš„å·®å¼‚
        result_uint8 = (result * 255).astype(np.uint8)
        for i, idx in enumerate(indices[:4]):
            mnist_img = x_test[idx]
            diff = cv2.absdiff(result_uint8, mnist_img)
            axes[1, 2 + i].imshow(diff, cmap='hot')
            axes[1, 2 + i].set_title(f'å·®å¼‚ #{idx}')
            axes[1, 2 + i].axis('off')
        
        plt.suptitle(f'ä¸MNISTæ ·æœ¬å¯¹æ¯” - {filename}', fontsize=14)
        plt.tight_layout()
        plt.show()


def main():
    """ä¸»å‡½æ•°"""
    print("=" * 60)
    print("ğŸ–ï¸  æ‰‹å†™æ•°å­—é¢„æµ‹ç³»ç»Ÿ")
    print("=" * 60)
    
    # åˆ›å»ºé¢„æµ‹å™¨
    predictor = HandwritingPredictor(
        model_path='mnist_cnn_model.h5',
        data_dir='my_handwriting_digits'
    )
    
    # åŠ è½½æ¨¡å‹
    if not predictor.load_model():
        return
    
    # å¯è§†åŒ–ä¸€å¼ æ•°å­—9çš„é¢„å¤„ç†è¿‡ç¨‹
    print("\nğŸ“Š é¢„å¤„ç†å¯è§†åŒ–...")
    test_file = '9_001.jpg'
    image_path = os.path.join(predictor.data_dir, test_file)
    if os.path.exists(image_path):
        print(f"\nå¤„ç†: {test_file}")
        predictor.visualize_preprocessing(image_path)
        predictor.compare_with_mnist(image_path)
    
    # æ‰¹é‡é¢„æµ‹
    print("\n" + "=" * 60)
    print("ğŸ“Š æ‰¹é‡é¢„æµ‹...")
    print("=" * 60)
    
    results, accuracy = predictor.predict_batch()
    
    # ä¿å­˜ç»“æœ
    if results:
        with open('prediction_results.txt', 'w', encoding='utf-8') as f:
            f.write("æ‰‹å†™æ•°å­—é¢„æµ‹ç»“æœ\n")
            f.write("=" * 50 + "\n")
            f.write(f"æ€»æ ·æœ¬: {len(results)}\n")
            f.write(f"å‡†ç¡®ç‡: {accuracy:.4f} ({accuracy*100:.2f}%)\n")
            f.write("=" * 50 + "\n\n")
            
            for r in results:
                status = 'âœ“' if r['correct'] else 'âœ—'
                f.write(f"{status} {r['filename']}: çœŸå®={r['true_label']}, "
                       f"é¢„æµ‹={r['pred_label']}, ç½®ä¿¡åº¦={r['confidence']:.3f}\n")
        
        print(f"\nâœ… ç»“æœå·²ä¿å­˜åˆ°: prediction_results.txt")
    
    print("\nğŸŠ é¢„æµ‹å®Œæˆ!")


if __name__ == "__main__":
    main()
