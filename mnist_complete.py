import tensorflow as tf
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.metrics import classification_report, confusion_matrix
import os
import time

# è®¾ç½®matplotlibæ˜¾ç¤ºä¸­æ–‡
plt.rcParams['font.sans-serif'] = ['SimHei']
plt.rcParams['axes.unicode_minus'] = False

class MNISTRecognition:
    def __init__(self):
        """åˆå§‹åŒ–MNISTæ‰‹å†™æ•°å­—è¯†åˆ«ç³»ç»Ÿ"""
        print("=== MNISTæ‰‹å†™æ•°å­—è¯†åˆ«ç³»ç»Ÿ ===")
        print("æ­£åœ¨åˆå§‹åŒ–...")
        
        # è®¾ç½®éšæœºç§å­ç¡®ä¿ç»“æœå¯é‡ç°
        tf.random.set_seed(42)
        np.random.seed(42)
        
        # åˆå§‹åŒ–å˜é‡
        self.model = None
        self.history = None
        self.x_train = None
        self.y_train = None
        self.x_test = None
        self.y_test = None
        
    def download_and_load_data(self):
        """ä¸‹è½½å¹¶åŠ è½½MNISTæ•°æ®é›†"""
        print("\n1. æ­£åœ¨ä¸‹è½½MNISTæ•°æ®é›†...")
        print("æ•°æ®æ¥æº: http://yann.lecun.com/exdb/mnist/")
        
        try:
            # TensorFlowä¼šè‡ªåŠ¨ä»å®˜æ–¹æºä¸‹è½½MNISTæ•°æ®
            (self.x_train, self.y_train), (self.x_test, self.y_test) = tf.keras.datasets.mnist.load_data()
            
            print(f"âœ… æ•°æ®ä¸‹è½½æˆåŠŸï¼")
            print(f"è®­ç»ƒé›†å¤§å°: {self.x_train.shape[0]} å¼ å›¾ç‰‡")
            print(f"æµ‹è¯•é›†å¤§å°: {self.x_test.shape[0]} å¼ å›¾ç‰‡")
            print(f"å›¾ç‰‡å°ºå¯¸: {self.x_train.shape[1]}Ã—{self.x_train.shape[2]} åƒç´ ")
            
            return True
            
        except Exception as e:
            print(f"âŒ æ•°æ®ä¸‹è½½å¤±è´¥: {str(e)}")
            print("è¯·æ£€æŸ¥ç½‘ç»œè¿æ¥æˆ–æ‰‹åŠ¨ä¸‹è½½æ•°æ®é›†")
            return False
    
    def preprocess_data(self):
        """æ•°æ®é¢„å¤„ç†"""
        print("\n2. æ­£åœ¨è¿›è¡Œæ•°æ®é¢„å¤„ç†...")
        
        # æ•°æ®æ ‡å‡†åŒ–: å°†åƒç´ å€¼ä» [0,255] ç¼©æ”¾åˆ° [0,1]
        self.x_train = self.x_train.astype('float32') / 255.0
        self.x_test = self.x_test.astype('float32') / 255.0
        
        # å¯¹äºCNNï¼Œéœ€è¦æ·»åŠ é€šé“ç»´åº¦
        self.x_train = self.x_train.reshape(self.x_train.shape[0], 28, 28, 1)
        self.x_test = self.x_test.reshape(self.x_test.shape[0], 28, 28, 1)
        
        # æ ‡ç­¾è½¬æ¢ä¸ºone-hotç¼–ç 
        self.y_train_categorical = tf.keras.utils.to_categorical(self.y_train, 10)
        self.y_test_categorical = tf.keras.utils.to_categorical(self.y_test, 10)
        
        print("âœ… æ•°æ®é¢„å¤„ç†å®Œæˆ")
        print(f"è®­ç»ƒæ•°æ®å½¢çŠ¶: {self.x_train.shape}")
        print(f"æµ‹è¯•æ•°æ®å½¢çŠ¶: {self.x_test.shape}")
        print(f"æ ‡ç­¾ç±»åˆ«æ•°: {len(np.unique(self.y_train))}")
    
    def visualize_samples(self):
        """å¯è§†åŒ–æ•°æ®æ ·æœ¬"""
        print("\n3. å¯è§†åŒ–æ•°æ®æ ·æœ¬...")
        
        # åˆ›å»ºå›¾å½¢
        plt.figure(figsize=(15, 8))
        
        # æ˜¾ç¤ºå‰20ä¸ªæ ·æœ¬
        for i in range(20):
            plt.subplot(2, 10, i + 1)
            plt.imshow(self.x_train[i].reshape(28, 28), cmap='gray')
            plt.title(f'æ ‡ç­¾: {self.y_train[i]}', fontsize=10)
            plt.axis('off')
        
        plt.suptitle('MNISTæ•°æ®é›†æ ·æœ¬å±•ç¤º', fontsize=16)
        plt.tight_layout()
        plt.show()
        
        # ç»Ÿè®¡å„ç±»åˆ«æ•°é‡
        unique, counts = np.unique(self.y_train, return_counts=True)
        
        plt.figure(figsize=(10, 6))
        plt.bar(unique, counts, color='skyblue', edgecolor='black')
        plt.xlabel('æ•°å­—ç±»åˆ«')
        plt.ylabel('æ ·æœ¬æ•°é‡')
        plt.title('è®­ç»ƒé›†ä¸­å„æ•°å­—ç±»åˆ«åˆ†å¸ƒ')
        plt.grid(True, alpha=0.3)
        
        # æ·»åŠ æ•°é‡æ ‡ç­¾
        for i, count in enumerate(counts):
            plt.text(i, count + 50, str(count), ha='center', va='bottom')
        
        plt.show()
    
    def build_cnn_model(self):
        """æ„å»ºCNNæ¨¡å‹ï¼ˆæ¨èï¼Œå‡†ç¡®ç‡æœ€é«˜ï¼‰"""
        print("\n4. æ„å»ºCNNæ¨¡å‹...")
        
        model = tf.keras.Sequential([
            # ç¬¬ä¸€ä¸ªå·ç§¯å±‚
            tf.keras.layers.Conv2D(32, (3, 3), activation='relu', input_shape=(28, 28, 1)),
            tf.keras.layers.MaxPooling2D((2, 2)),
            
            # ç¬¬äºŒä¸ªå·ç§¯å±‚
            tf.keras.layers.Conv2D(64, (3, 3), activation='relu'),
            tf.keras.layers.MaxPooling2D((2, 2)),
            
            # ç¬¬ä¸‰ä¸ªå·ç§¯å±‚
            tf.keras.layers.Conv2D(64, (3, 3), activation='relu'),
            
            # å±•å¹³å±‚
            tf.keras.layers.Flatten(),
            
            # å…¨è¿æ¥å±‚
            tf.keras.layers.Dense(64, activation='relu'),
            tf.keras.layers.Dropout(0.5),  # é˜²æ­¢è¿‡æ‹Ÿåˆ
            
            # è¾“å‡ºå±‚
            tf.keras.layers.Dense(10, activation='softmax')  # 10ä¸ªç±»åˆ«
        ])
        
        # ç¼–è¯‘æ¨¡å‹
        model.compile(
            optimizer='adam',
            loss='categorical_crossentropy',
            metrics=['accuracy']
        )
        
        # æ˜¾ç¤ºæ¨¡å‹ç»“æ„
        model.summary()
        
        self.model = model
        print("âœ… CNNæ¨¡å‹æ„å»ºå®Œæˆ")
    
    def train_model(self, epochs=12, batch_size=128):
        """è®­ç»ƒæ¨¡å‹"""
        print(f"\n5. å¼€å§‹è®­ç»ƒæ¨¡å‹ (epochs={epochs}, batch_size={batch_size})...")
        
        # è®¾ç½®å›è°ƒå‡½æ•°
        callbacks = [
            tf.keras.callbacks.EarlyStopping(
                monitor='val_accuracy',
                patience=3,
                restore_best_weights=True
            ),
            tf.keras.callbacks.ReduceLROnPlateau(
                monitor='val_loss',
                factor=0.2,
                patience=2,
                min_lr=0.0001
            )
        ]
        
        start_time = time.time()
        
        # è®­ç»ƒæ¨¡å‹
        self.history = self.model.fit(
            self.x_train, self.y_train_categorical,
            batch_size=batch_size,
            epochs=epochs,
            validation_data=(self.x_test, self.y_test_categorical),
            callbacks=callbacks,
            verbose=1
        )
        
        training_time = time.time() - start_time
        
        print(f"âœ… æ¨¡å‹è®­ç»ƒå®Œæˆï¼Œè€—æ—¶: {training_time:.2f} ç§’")
        
        # è·å–æœ€ä½³å‡†ç¡®ç‡
        best_val_accuracy = max(self.history.history['val_accuracy'])
        print(f"ğŸ¯ æœ€ä½³éªŒè¯å‡†ç¡®ç‡: {best_val_accuracy:.4f} ({best_val_accuracy*100:.2f}%)")
        
        return best_val_accuracy
    
    def evaluate_model(self):
        """è¯„ä¼°æ¨¡å‹æ€§èƒ½"""
        print("\n6. è¯„ä¼°æ¨¡å‹æ€§èƒ½...")
        
        # åœ¨æµ‹è¯•é›†ä¸Šè¯„ä¼°
        test_loss, test_accuracy = self.model.evaluate(self.x_test, self.y_test_categorical, verbose=0)
        
        print(f"ğŸ“Š æµ‹è¯•é›†æ€§èƒ½:")
        print(f"  - æŸå¤±å€¼: {test_loss:.4f}")
        print(f"  - å‡†ç¡®ç‡: {test_accuracy:.4f} ({test_accuracy*100:.2f}%)")
        
        # æ£€æŸ¥æ˜¯å¦è¾¾åˆ°è¦æ±‚
        if test_accuracy >= 0.96:
            print(f"ğŸ‰ æ­å–œï¼æ¨¡å‹å‡†ç¡®ç‡ {test_accuracy*100:.2f}% è¾¾åˆ°è¦æ±‚ (â‰¥96%)")
        else:
            print(f"âš ï¸ æ¨¡å‹å‡†ç¡®ç‡ {test_accuracy*100:.2f}% æœªè¾¾åˆ°è¦æ±‚ (â‰¥96%)")
            print("å»ºè®®ï¼šå¢åŠ è®­ç»ƒè½®æ•°æˆ–è°ƒæ•´æ¨¡å‹ç»“æ„")
        
        return test_accuracy
    
    def detailed_analysis(self):
        """è¯¦ç»†åˆ†ææ¨¡å‹æ€§èƒ½"""
        print("\n7. è¯¦ç»†æ€§èƒ½åˆ†æ...")
        
        # é¢„æµ‹æµ‹è¯•é›†
        y_pred = self.model.predict(self.x_test)
        y_pred_classes = np.argmax(y_pred, axis=1)
        
        # åˆ†ç±»æŠ¥å‘Š
        print("\nåˆ†ç±»æŠ¥å‘Š:")
        print(classification_report(self.y_test, y_pred_classes))
        
        # æ··æ·†çŸ©é˜µå¯è§†åŒ–
        cm = confusion_matrix(self.y_test, y_pred_classes)
        
        plt.figure(figsize=(10, 8))
        sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', 
                   xticklabels=range(10), yticklabels=range(10))
        plt.title('æ··æ·†çŸ©é˜µ')
        plt.xlabel('é¢„æµ‹æ ‡ç­¾')
        plt.ylabel('çœŸå®æ ‡ç­¾')
        plt.show()
        
        # è®¡ç®—æ¯ä¸ªç±»åˆ«çš„å‡†ç¡®ç‡
        class_accuracy = cm.diagonal() / cm.sum(axis=1)
        
        plt.figure(figsize=(10, 6))
        bars = plt.bar(range(10), class_accuracy, color='lightgreen', edgecolor='black')
        plt.xlabel('æ•°å­—ç±»åˆ«')
        plt.ylabel('å‡†ç¡®ç‡')
        plt.title('å„æ•°å­—ç±»åˆ«è¯†åˆ«å‡†ç¡®ç‡')
        plt.ylim(0, 1)
        plt.grid(True, alpha=0.3)
        
        # æ·»åŠ æ•°å€¼æ ‡ç­¾
        for i, bar in enumerate(bars):
            height = bar.get_height()
            plt.text(bar.get_x() + bar.get_width()/2., height + 0.01,
                    f'{height:.3f}', ha='center', va='bottom')
        
        plt.show()
    
    def plot_training_history(self):
        """ç»˜åˆ¶è®­ç»ƒå†å²"""
        if self.history is None:
            print("æ²¡æœ‰è®­ç»ƒå†å²æ•°æ®")
            return
        
        print("\n8. ç»˜åˆ¶è®­ç»ƒå†å²...")
        
        fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(15, 5))
        
        # å‡†ç¡®ç‡æ›²çº¿
        ax1.plot(self.history.history['accuracy'], label='è®­ç»ƒå‡†ç¡®ç‡', marker='o')
        ax1.plot(self.history.history['val_accuracy'], label='éªŒè¯å‡†ç¡®ç‡', marker='s')
        ax1.set_title('æ¨¡å‹å‡†ç¡®ç‡å˜åŒ–')
        ax1.set_xlabel('è½®æ¬¡')
        ax1.set_ylabel('å‡†ç¡®ç‡')
        ax1.legend()
        ax1.grid(True, alpha=0.3)
        
        # æŸå¤±æ›²çº¿
        ax2.plot(self.history.history['loss'], label='è®­ç»ƒæŸå¤±', marker='o')
        ax2.plot(self.history.history['val_loss'], label='éªŒè¯æŸå¤±', marker='s')
        ax2.set_title('æ¨¡å‹æŸå¤±å˜åŒ–')
        ax2.set_xlabel('è½®æ¬¡')
        ax2.set_ylabel('æŸå¤±')
        ax2.legend()
        ax2.grid(True, alpha=0.3)
        
        plt.tight_layout()
        plt.show()
    
    def predict_samples(self, num_samples=10):
        """é¢„æµ‹éšæœºæ ·æœ¬å¹¶å¯è§†åŒ–"""
        print(f"\n9. é¢„æµ‹ {num_samples} ä¸ªéšæœºæ ·æœ¬...")
        
        # éšæœºé€‰æ‹©æ ·æœ¬
        indices = np.random.choice(len(self.x_test), num_samples, replace=False)
        
        plt.figure(figsize=(15, 6))
        
        for i, idx in enumerate(indices):
            # é¢„æµ‹
            img = self.x_test[idx:idx+1]
            pred = self.model.predict(img, verbose=0)
            pred_class = np.argmax(pred)
            confidence = np.max(pred)
            true_class = self.y_test[idx]
            
            # æ˜¾ç¤ºå›¾åƒ
            plt.subplot(2, 5, i + 1)
            plt.imshow(img.reshape(28, 28), cmap='gray')
            
            # è®¾ç½®æ ‡é¢˜é¢œè‰²ï¼ˆæ­£ç¡®ä¸ºç»¿è‰²ï¼Œé”™è¯¯ä¸ºçº¢è‰²ï¼‰
            color = 'green' if pred_class == true_class else 'red'
            plt.title(f'çœŸå®:{true_class} é¢„æµ‹:{pred_class}\nç½®ä¿¡åº¦:{confidence:.3f}', 
                     color=color, fontsize=10)
            plt.axis('off')
        
        plt.suptitle('éšæœºæ ·æœ¬é¢„æµ‹ç»“æœ (ç»¿è‰²=æ­£ç¡®, çº¢è‰²=é”™è¯¯)', fontsize=14)
        plt.tight_layout()
        plt.show()
    
    def save_model(self, filename='mnist_model.h5'):
        """ä¿å­˜æ¨¡å‹"""
        if self.model is None:
            print("æ²¡æœ‰è®­ç»ƒå¥½çš„æ¨¡å‹å¯ä¿å­˜")
            return
        
        self.model.save(filename)
        print(f"âœ… æ¨¡å‹å·²ä¿å­˜ä¸º: {filename}")
    
    def run_complete_pipeline(self):
        """è¿è¡Œå®Œæ•´çš„è®­ç»ƒå’Œè¯„ä¼°æµç¨‹"""
        print("ğŸš€ å¼€å§‹MNISTæ‰‹å†™æ•°å­—è¯†åˆ«å®Œæ•´æµç¨‹...")
        
        # 1. ä¸‹è½½æ•°æ®
        if not self.download_and_load_data():
            return False
        
        # 2. é¢„å¤„ç†
        self.preprocess_data()
        
        # 3. å¯è§†åŒ–æ ·æœ¬
        self.visualize_samples()
        
        # 4. æ„å»ºæ¨¡å‹
        self.build_cnn_model()
        
        # 5. è®­ç»ƒæ¨¡å‹
        best_accuracy = self.train_model(epochs=12)
        
        # 6. è¯„ä¼°æ¨¡å‹
        final_accuracy = self.evaluate_model()
        
        # 7. è¯¦ç»†åˆ†æ
        self.detailed_analysis()
        
        # 8. ç»˜åˆ¶è®­ç»ƒå†å²
        self.plot_training_history()
        
        # 9. é¢„æµ‹æ ·æœ¬
        self.predict_samples()
        
        # 10. ä¿å­˜æ¨¡å‹
        self.save_model()
        
        # æœ€ç»ˆæŠ¥å‘Š
        print("\n" + "="*50)
        print("ğŸ“‹ æœ€ç»ˆæŠ¥å‘Š")
        print("="*50)
        print(f"ğŸ¯ æœ€ç»ˆæµ‹è¯•å‡†ç¡®ç‡: {final_accuracy*100:.2f}%")
        print(f"ğŸ“ˆ è¦æ±‚å‡†ç¡®ç‡: 96.00%")
        
        if final_accuracy >= 0.96:
            print("ğŸ‰ ä»»åŠ¡å®Œæˆï¼æ¨¡å‹è¾¾åˆ°è¦æ±‚")
        else:
            print("âš ï¸ æœªè¾¾åˆ°è¦æ±‚ï¼Œå»ºè®®è°ƒæ•´å‚æ•°æˆ–æ¨¡å‹ç»“æ„")
        
        print("="*50)
        
        return final_accuracy >= 0.96

def main():
    """ä¸»å‡½æ•°"""
    # åˆ›å»ºè¯†åˆ«ç³»ç»Ÿ
    mnist_system = MNISTRecognition()
    
    # è¿è¡Œå®Œæ•´æµç¨‹
    success = mnist_system.run_complete_pipeline()
    
    if success:
        print("\nğŸŠ MNISTæ‰‹å†™æ•°å­—è¯†åˆ«ä»»åŠ¡æˆåŠŸå®Œæˆï¼")
    else:
        print("\nğŸ”„ å¦‚éœ€æé«˜å‡†ç¡®ç‡ï¼Œå»ºè®®å¢åŠ è®­ç»ƒè½®æ•°æˆ–è°ƒæ•´æ¨¡å‹")

if __name__ == "__main__":
    main()